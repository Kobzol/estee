import collections
import itertools
import multiprocessing
import os
import random
import re
import signal
import sys
import threading
import time
import traceback
from typing import List

import click
import numpy
import pandas as pd
from distributed import as_completed
from tqdm import tqdm

from estee.common import imode
from estee.schedulers import WorkStealingScheduler
from estee.schedulers.basic import AllOnOneScheduler, RandomAssignScheduler
from estee.schedulers.camp import Camp2Scheduler
from estee.schedulers.clustering import LcScheduler
from estee.schedulers.genetic import GeneticScheduler
from estee.schedulers.others import BlevelScheduler, DLSScheduler, ETFScheduler, MCPGTScheduler, \
    MCPScheduler, TlevelScheduler, RandomHeuristicScheduler
from estee.schedulers.queue import BlevelGtScheduler, RandomGtScheduler, TlevelGtScheduler
from estee.serialization.dask_json import json_deserialize, json_serialize
from estee.simulator import MaxMinFlowNetModel, SimpleNetModel
from estee.simulator import Simulator, Worker
from estee.simulator.trace import FetchEndTraceEvent

SCHEDULERS = {
    "single": AllOnOneScheduler,
    "blevel": BlevelScheduler,
    "blevel-gt": BlevelGtScheduler,
    "tlevel": TlevelScheduler,
    "tlevel-gt": TlevelGtScheduler,
    "random": RandomAssignScheduler,
    "random-heuristic": RandomHeuristicScheduler,
    "random-gt": RandomGtScheduler,
    "dls": DLSScheduler,
    "etf": ETFScheduler,
    "mcp": MCPScheduler,
    "mcp-gt": MCPGTScheduler,
    "genetic": GeneticScheduler,
    "camp2": lambda: Camp2Scheduler(5000),
    "lc": LcScheduler,
    "ws": WorkStealingScheduler
}

NETMODELS = {
    "simple": SimpleNetModel,
    "maxmin": MaxMinFlowNetModel
}

CLUSTERS = {
    "2x8": [{"cpus": 8}] * 2,
    "4x4": [{"cpus": 4}] * 4,
    "8x4": [{"cpus": 4}] * 8,
    "16x4": [{"cpus": 4}] * 16,
    "32x4": [{"cpus": 4}] * 32,
    "8x8": [{"cpus": 8}] * 8,
    "16x8": [{"cpus": 8}] * 16,
    "32x8": [{"cpus": 8}] * 32,
    "stairs16": [{"cpus": i} for i in range(1, 6)] + [{"cpus": 1}],
    "8x16": [{"cpus": 16}] * 8,
    "16x16": [{"cpus": 16}] * 16,
    "32x16": [{"cpus": 16}] * 32,
    "64x16": [{"cpus": 16}] * 64,
    "128x16": [{"cpus": 16}] * 128,
    "256x16": [{"cpus": 16}] * 256,
}

BANDWIDTHS = {
    "8G": 8192,
    "2G": 2048,
    "512M": 512,
    "128M": 128,
    "32M": 32
}

IMODES = {
    "exact": imode.process_imode_exact,
    "blind": imode.process_imode_blind,
    "mean": imode.process_imode_mean,
    "user": imode.process_imode_user,
}

SCHED_TIMINGS = {
    # min_sched_interval, sched_time
    "0/0": (0, 0),
    "0.1/0.05": (0.1, 0.05),
    "0.4/0.05": (0.4, 0.05),
    "1.6/0.05": (1.6, 0.05),
    "6.4/0.05": (6.4, 0.05)
}

COLUMNS = ["graph_set",
           "graph_name",
           "graph_id",
           "cluster_name",
           "bandwidth",
           "netmodel",
           "scheduler_name",
           "imode",
           "min_sched_interval",
           "sched_time",
           "time",
           "execution_time",
           "total_transfer"]


def generate_seed():
    seed = os.getpid() * time.time()
    for b in os.urandom(4):
        seed *= b
    seed = int(seed) % 2**32
    random.seed(seed)
    numpy.random.seed(seed)


def init_worker():
    signal.signal(signal.SIGINT, signal.SIG_IGN)


generate_seed()


BenchmarkInstance = collections.namedtuple(
    "BenchmarkInstance", ("id", "graph_set", "graph_name", "graph_id", "graph",
                          "cluster_name", "bandwidth", "netmodel",
                          "scheduler_name", "imode", "min_sched_interval", "sched_time",
                          "count"))

BenchmarkResult = collections.namedtuple(
    "BenchmarkResult", ("makespan", "runtime", "transfer"))


class BenchmarkConfig:
    graph_cache = {}

    def __init__(self, graph_frame, schedulers, clusters, netmodels, bandwidths,
                 imodes, sched_timings, count):
        self.graph_frame = graph_frame
        self.schedulers = schedulers
        self.clusters = clusters
        self.netmodels = netmodels
        self.bandwidths = bandwidths
        self.imodes = imodes
        self.sched_timings = sched_timings
        self.count = count

    def generate_instances(self):
        def calculate_imodes(graph, graph_id):
            if graph_id not in BenchmarkConfig.graph_cache:
                BenchmarkConfig.graph_cache[graph_id] = {}
                for mode in IMODES:
                    g = json_deserialize(graph)
                    IMODES[mode](g)
                    BenchmarkConfig.graph_cache[graph_id][mode] = json_serialize(g)

        for graph_def, cluster_name, bandwidth, netmodel, scheduler_name, mode, sched_timing \
                in itertools.product(self.graph_frame.iterrows(), self.clusters, self.bandwidths,
                                     self.netmodels, self.schedulers, self.imodes,
                                     self.sched_timings):
            g = graph_def[1]
            calculate_imodes(g["graph"], g["graph_id"])
            graph = BenchmarkConfig.graph_cache[g["graph_id"]][mode]

            (min_sched_interval, sched_time) = SCHED_TIMINGS[sched_timing]
            instance = BenchmarkInstance(0,
                g["graph_set"], g["graph_name"], g["graph_id"], graph,
                cluster_name, BANDWIDTHS[bandwidth], netmodel,
                scheduler_name,
                mode,
                min_sched_interval, sched_time,
                self.count)
            yield instance

    def __repr__(self):
        return """
============ Config ========================
scheduler : {schedulers}
cluster   : {clusters}
netmodel  : {netmodels}
bandwidths: {bandwidths}
imode     : {imodes}
timings   : {timings}
REPEAT    : {repeat}
============================================
        """.format(
            schedulers=", ".join(self.schedulers),
            clusters=", ".join(self.clusters),
            netmodels=", ".join(self.netmodels),
            bandwidths=", ".join(self.bandwidths),
            imodes=", ".join(self.imodes),
            timings=", ".join(self.sched_timings),
            repeat=self.count
        )


class BenchmarkEnvironment:
    def create_iterator(self, instances: List[BenchmarkInstance]):
        raise NotImplementedError()

    def postprocess_result(self, result):
        return result


class ProcessEnvironment(BenchmarkEnvironment):
    def __init__(self):
        self.pool = multiprocessing.Pool(initializer=init_worker)

    def create_iterator(self, instances):
        return self.pool.imap(process_multiprocessing, instances)


class DaskEnviroment(BenchmarkEnvironment):
    def __init__(self, dask_cluster):
        from dask.distributed import Client

        self.client = Client(dask_cluster)

    def create_iterator(self, instances: List[BenchmarkInstance]):
        graphs = {}
        instance_to_graph = {}
        instances = list(instances)
        for (i, instance) in enumerate(instances):
            if instance.graph not in graphs:
                graphs[instance.graph] = self.client.scatter([instance.graph], broadcast=True)[0]
            inst = instance._replace(graph=None)
            instance_to_graph[inst] = graphs[instance.graph]
            instances[i] = inst

        return as_completed(
            self.client.map(process_dask, ((instance_to_graph[i], i) for i in instances)))

    def postprocess_result(self, result):
        return result.result()


def run_single_instance(instance: BenchmarkInstance) -> BenchmarkResult:
    inf = 2**32

    def create_worker(wargs):
        if instance.netmodel == "simple":
            return Worker(**wargs, max_downloads=inf, max_downloads_per_worker=inf)
        return Worker(**wargs)

    begin_time = time.monotonic()
    workers = [create_worker(wargs) for wargs in CLUSTERS[instance.cluster_name]]
    netmodel = NETMODELS[instance.netmodel](instance.bandwidth)
    scheduler = SCHEDULERS[instance.scheduler_name]()
    simulator = Simulator(instance.graph, workers, scheduler, netmodel, trace=True)

    try:
        sim_time = simulator.run()
        runtime = time.monotonic() - begin_time
        transfer = 0
        for e in simulator.trace_events:
            if isinstance(e, FetchEndTraceEvent):
                transfer += e.output.size
        return BenchmarkResult(sim_time, runtime, transfer)
    except:
        traceback.print_exc()
        print("ERROR INSTANCE: {}".format(instance), file=sys.stderr)
        return None


def benchmark_scheduler(instance):
    return (instance.id, [run_single_instance(instance)
            for _ in range(instance.count)])


def process_multiprocessing(instance):
    instance = instance._replace(graph=json_deserialize(instance.graph))
    return benchmark_scheduler(instance)


def process_dask(conf):
    (graph, instance) = conf
    instance = instance._replace(graph=json_deserialize(graph))
    return benchmark_scheduler(instance)


def compute(instances, timeout=0, dask_cluster=None):
    rows = []

    if not instances:
        return rows

    for i, instance in enumerate(instances):
        instances[i] = instance._replace(id=i)

    if dask_cluster:
        env = DaskEnviroment(dask_cluster)
    else:
        env = ProcessEnvironment()

    if timeout:
        print("Timeout set to {} seconds".format(timeout))

    def run():
        counter = 0
        try:
            iterator = env.create_iterator(instances)
            for item in tqdm(iterator, total=len(instances)):
                counter += 1

                (id, results) = env.postprocess_result(item)
                instance = instances[id]
                for result in results:
                    if result is not None:
                        rows.append((
                            instance.graph_set,
                            instance.graph_name,
                            instance.graph_id,
                            instance.cluster_name,
                            instance.bandwidth,
                            instance.netmodel,
                            instance.scheduler_name,
                            instance.imode,
                            instance.min_sched_interval,
                            instance.sched_time,
                            result.makespan,
                            result.runtime,
                            result.transfer
                        ))
        except:
            traceback.print_exc()
            print("Benchmark interrupted, iterated {} instances. Writing intermediate results"
                  .format(counter))

    if timeout:
        thread = threading.Thread(target=run)
        thread.daemon = True
        thread.start()
        thread.join(timeout)
        if thread.is_alive():
            print("Timeout reached")
    else:
        run()

    return rows


def run_benchmark(configs, oldframe, resultfile, skip_completed, timeout=0, dask_cluster=None):
    for config in configs:
        print(config)

    instances = create_instances(configs, oldframe, skip_completed, 5)
    rows = compute(instances, timeout, dask_cluster)
    if not rows:
        print("No results were computed")
        return

    frame = pd.DataFrame(rows, columns=COLUMNS)
    print(frame.groupby(["graph_name", "graph_id", "cluster_name",
                         "bandwidth", "netmodel", "imode", "min_sched_interval", "sched_time",
                         "scheduler_name"]).mean())

    if len(frame) > 0:
        base, ext = os.path.splitext(resultfile)
        path = "{}.backup{}".format(base, ext)
        print("Creating backup of old results to '{}'".format(path))
        write_resultfile(oldframe, path)

    newframe = pd.concat([oldframe, frame], ignore_index=True)
    write_resultfile(newframe, resultfile)
    print("{} entries in new '{}'".format(newframe["time"].count(), resultfile))


def skip_completed_instances(instances, frame, repeat, columns, batch):
    skipped_output = 0
    skipped_batch = 0
    counts = frame.groupby(columns).size()

    result = []
    for instance in instances:
        hashed = tuple(getattr(instance, col) for col in columns)
        existing_count = 0
        if hashed in counts:
            count = counts.loc[hashed]
            skipped_output += count
            existing_count += count
        if hashed in batch:
            count = batch[hashed]
            skipped_batch += count
            existing_count += count
        if existing_count == 0:
            result.append(instance)
        elif existing_count < repeat:
            result.append(instance._replace(count=repeat - existing_count))

    if skipped_output or skipped_batch:
        print("Skipping {} instances from output, {} from batch, {} left".format(skipped_output,
                                                                                 skipped_batch,
                                                                                 len(result)))
    return result


def limit_max_count(instances, max_count):
    result = []
    for instance in instances:
        if instance.count > max_count:
            remaining = instance.count
            while remaining > 0:
                count = min(max_count, remaining)
                remaining -= count
                result.append(instance._replace(count=count))
        else:
            result.append(instance)

    return result


def create_instances(configs, frame, skip_completed, max_count):
    total_instances = []
    columns = ["graph_id",
               "cluster_name",
               "bandwidth",
               "netmodel",
               "scheduler_name",
               "imode",
               "min_sched_interval",
               "sched_time"]
    batch = {}

    for config in configs:
        instances = list(config.generate_instances())
        if skip_completed:
            instances = skip_completed_instances(instances, frame, config.count, columns, batch)
        instances = limit_max_count(instances, max_count)
        for instance in instances:
            hashed = tuple(getattr(instance, col) for col in columns)
            batch[hashed] = instance.count + batch.get(hashed, 0)
        total_instances += instances

    return total_instances


def load_resultfile(resultfile, append):
    if os.path.isfile(resultfile):
        if not append:
            print("Result file '{}' already exists\n"
                  "Remove --no-append to append results to it".format(resultfile),
                  file=sys.stderr)
            exit(1)

        print("Appending to result file '{}'".format(resultfile))

        oldframe = pd.read_csv(resultfile)
        assert list(oldframe.columns) == COLUMNS
    else:
        print("Creating result file '{}'".format(resultfile))
        oldframe = pd.DataFrame([], columns=COLUMNS)
    return oldframe


def write_resultfile(frame, resultfile):
    frame.to_csv(resultfile, compression='zip', index=False)


def load_graphs(graphsets, graph_names=None):
    frame = pd.DataFrame()
    for path in graphsets:
        graph = pd.read_json(path)
        graph.insert(loc=0, column='graph_set', value=os.path.splitext(path)[0])
        frame = pd.concat([frame, graph], ignore_index=True)

    if graph_names:
        frame = frame[frame["graph_name"].isin(graph_names)].reset_index()
    return frame


def generate_help(keys):
    return "all,{}".format(",".join(keys))


def parse_timeout(timeout):
    if not timeout:
        return 0
    match = re.match(r"^(\d{2}):(\d{2}):(\d{2})$", timeout)
    if not match:
        print("Wrong timeout format. Enter timeout as hh:mm:ss.")
        exit(1)
    return int(match.group(1)) * 3600 + int(match.group(2)) * 60 + int(match.group(3))


@click.command()
@click.argument("graphset")
@click.argument("resultfile")
@click.option("--scheduler", default="all", help=generate_help(SCHEDULERS))
@click.option("--cluster", default="all", help=generate_help(CLUSTERS))
@click.option("--bandwidth", default="all", help=generate_help(BANDWIDTHS))
@click.option("--netmodel", default="all", help=generate_help(NETMODELS))
@click.option("--imode", default="all", help=generate_help(IMODES))
@click.option("--sched-timing", default="all", help=generate_help(SCHED_TIMINGS))
@click.option("--repeat", default=1)
@click.option("--append/--no-append", default=True, help="Exit if the resultfile already exists.")
@click.option("--skip-completed/--no-skip_completed", default=True,
              help="Skip already computed instances found in the resultfile.")
@click.option("--graphs", help="Comma separated list of graphs to be used from the input graphset")
@click.option("--timeout", help="Timeout for the computation. Format hh:mm:ss.")
@click.option("--dask-cluster", help="Address of Dask scheduler.")
def compute_cmd(graphset, resultfile, scheduler, cluster, bandwidth,
                netmodel, imode, sched_timing, repeat, append, skip_completed,
                graphs, timeout, dask_cluster):
    def parse_option(value, keys):
        if value == "all":
            return list(keys)
        value = [v.strip() for v in value.split(",")]
        assert all(v in keys for v in value)
        return value

    graphsets = graphset.split(",")
    schedulers = parse_option(scheduler, SCHEDULERS)
    clusters = parse_option(cluster, CLUSTERS)
    bandwidths = parse_option(bandwidth, BANDWIDTHS)
    netmodels = parse_option(netmodel, NETMODELS)
    imodes = parse_option(imode, IMODES)
    sched_timings = parse_option(sched_timing, SCHED_TIMINGS)
    timeout = parse_timeout(timeout)

    graph_frame = load_graphs(graphsets, None if graphs is None else graphs.split(","))
    if len(graph_frame) == 0:
        print("No graphs selected")
        exit()

    config = BenchmarkConfig(graph_frame, schedulers, clusters, netmodels, bandwidths, imodes,
                             sched_timings, repeat)
    frame = load_resultfile(resultfile, append)

    run_benchmark([config], frame, resultfile, skip_completed, timeout, dask_cluster)


if __name__ == "__main__":
    compute_cmd()
